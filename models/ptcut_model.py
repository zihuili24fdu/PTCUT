"""PTCUT Model (Prompt-Tuned CUT)

åŸºäºCUTæ¨¡å‹çš„è™šæ‹ŸæŸ“è‰²æ¨¡å‹,ç»“åˆKgCoOpè®­ç»ƒå¥½çš„æç¤ºç‰¹å¾å’ŒCONCHè§†è§‰ç¼–ç å™¨

============================================================================
æ ¸å¿ƒæ€æƒ³ï¼šä½¿ç”¨CONCHä½œä¸º"è¯­ä¹‰ç›‘ç£å™¨"
============================================================================

ä¼ ç»ŸCUTæ¨¡å‹çš„é—®é¢˜ï¼š
- ä»…ä½¿ç”¨GANæŸå¤±å’ŒNCEå¯¹æ¯”æŸå¤±
- ç¼ºä¹æ˜ç¡®çš„è¯­ä¹‰ï¼ˆåˆ†ç±»ï¼‰ç›‘ç£
- ç”Ÿæˆå›¾åƒå¯èƒ½åœ¨è§†è§‰ä¸Šç›¸ä¼¼,ä½†è¯­ä¹‰ä¿¡æ¯ä¸¢å¤±

PTCUTçš„è§£å†³æ–¹æ¡ˆï¼š
æ·»åŠ ä¸¤ä¸ªåŸºäºCONCHçš„è¯­ä¹‰æŸå¤±ï¼š

1. åˆ†ç±»æŸå¤± (Classification Loss):
   - ä½¿ç”¨CONCHè§†è§‰ç¼–ç å™¨æå–ç”Ÿæˆå›¾åƒ(fakeB)çš„ç‰¹å¾
   - ä¸KgCoOpè®­ç»ƒçš„æ–‡æœ¬ç‰¹å¾è®¡ç®—ç›¸ä¼¼åº¦
   - ç¡®ä¿ç”Ÿæˆå›¾åƒå…·æœ‰æ­£ç¡®çš„è¯­ä¹‰ç±»åˆ«
   
2. è’¸é¦æŸå¤± (Distillation Loss):
   - ä½¿ç”¨CONCHåŒæ—¶ç¼–ç çœŸå®å›¾åƒ(realB)å’Œç”Ÿæˆå›¾åƒ(fakeB)
   - çº¦æŸä¸¤è€…åœ¨CONCHç‰¹å¾ç©ºé—´ä¸­ç›¸ä¼¼
   - ç¡®ä¿è¯­ä¹‰ä¿¡æ¯ä»çœŸå®å›¾åƒä¼ é€’åˆ°ç”Ÿæˆå›¾åƒ

å®Œæ•´æŸå¤±å‡½æ•°ï¼š
loss_total = loss_GAN + loss_NCE + Î»_cls * loss_CLS + Î»_distill * loss_DISTILL
             ^^^^^^^^   ^^^^^^^^^   ^^^^^^^^^^^^^^^^^^   ^^^^^^^^^^^^^^^^^^^^^^^^
             å¯¹æŠ—æŸå¤±    å¯¹æ¯”æŸå¤±    åˆ†ç±»ç›‘ç£(æ–°)         çŸ¥è¯†è’¸é¦(æ–°)

CONCHåœ¨PTCUTä¸­çš„è§’è‰²ï¼š
- âœ… æä¾›é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨ï¼ˆå†»ç»“,ä¸è®­ç»ƒï¼‰
- âœ… ä½œä¸º"è¯­ä¹‰è¯„ä¼°å™¨"æŒ‡å¯¼ç”Ÿæˆå™¨å­¦ä¹ 
- âœ… ç¡®ä¿ç”Ÿæˆå›¾åƒä¿æŒæ­£ç¡®çš„ç—…ç†å­¦ç±»åˆ«ç‰¹å¾
- âœ… logit_scaleæä¾›æ­£ç¡®çš„ç›¸ä¼¼åº¦æ¸©åº¦å‚æ•°
"""

import random
import numpy as np
import torch
import torch.nn as nn
from torch.nn import functional as F
from torchvision import transforms
from .base_model import BaseModel
from . import networks
from .patchnce import PatchNCELoss
import util.util as util
import os.path as osp
import sys

# ============================================================================
# å¯¼å…¥CONCHæ¨¡å‹
# ============================================================================
# CONCHæä¾›é¢„è®­ç»ƒçš„è§†è§‰-è¯­è¨€ç¼–ç å™¨,ç”¨äºæå–å›¾åƒçš„è¯­ä¹‰ç‰¹å¾
conch_path = "/home/lzh/myCode/CONCH"
if conch_path not in sys.path:
    sys.path.insert(0, conch_path)
from conch.open_clip_custom import create_model_from_pretrained


def build_conch_preprocess(image_size=448):
    """
    æ„å»º CONCH/CLIP çš„æ­£ç¡®é¢„å¤„ç†æµç¨‹
    
    å…³é”®ï¼š
    - ä½¿ç”¨ Resize(smaller_edge) + CenterCropï¼Œè€Œä¸æ˜¯ç›´æ¥ Resize
    - ä½¿ç”¨ CLIP/CONCH çš„ mean/stdï¼Œè€Œä¸æ˜¯ ImageNet çš„
    """
    return transforms.Compose([
        transforms.Resize(image_size, interpolation=transforms.InterpolationMode.BICUBIC),
        transforms.CenterCrop(image_size),
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.48145466, 0.4578275, 0.40821073],  # CLIP/CONCH mean
            std=[0.26862954, 0.26130258, 0.27577711]   # CLIP/CONCH std
        ),
    ])


class PTCUTModel(BaseModel):
    """
    PTCUTæ¨¡å‹ç±»ï¼šåŸºäºCUTæ¨¡å‹çš„æç¤ºè°ƒä¼˜è™šæ‹ŸæŸ“è‰²æ¨¡å‹
    
    åœ¨CUTæ¨¡å‹çš„åŸºç¡€ä¸Šæ·»åŠ ï¼š
    - CONCHè§†è§‰ç¼–ç å™¨ç”¨äºç‰¹å¾æå–
    - é¢„è®­ç»ƒçš„æ–‡æœ¬æç¤ºç‰¹å¾ç”¨äºåˆ†ç±»
    - åˆ†ç±»æŸå¤±å’Œè’¸é¦æŸå¤±
    """
    
    @staticmethod
    def modify_commandline_options(parser, is_train=True):
        """é…ç½®PTCUTæ¨¡å‹ç‰¹æœ‰çš„å‚æ•°é€‰é¡¹"""
        # ç»§æ‰¿CUTæ¨¡å‹çš„æ‰€æœ‰å‚æ•°
        parser.add_argument('--CUT_mode', type=str, default="CUT", choices='(CUT, cut, FastCUT, fastcut)')
        parser.add_argument('--lambda_GAN', type=float, default=1.0, help='GANæŸå¤±çš„æƒé‡')
        parser.add_argument('--lambda_NCE', type=float, default=1.0, help='NCEæŸå¤±çš„æƒé‡')
        parser.add_argument('--nce_idt', type=util.str2bool, nargs='?', const=True, default=False, 
                          help='æ˜¯å¦å¯¹identityæ˜ å°„ä½¿ç”¨NCEæŸå¤±')
        parser.add_argument('--nce_layers', type=str, default='0,4,8,12,16', help='åœ¨å“ªäº›å±‚ä¸Šè®¡ç®—NCEæŸå¤±')
        parser.add_argument('--nce_includes_all_negatives_from_minibatch',
                          type=util.str2bool, nargs='?', const=True, default=False,
                          help='è®¡ç®—å¯¹æ¯”æŸå¤±æ—¶æ˜¯å¦åŒ…å«minibatchä¸­å…¶ä»–æ ·æœ¬çš„è´Ÿæ ·æœ¬')
        parser.add_argument('--netF', type=str, default='mlp_sample', 
                          choices=['sample', 'reshape', 'mlp_sample'], help='ç‰¹å¾å›¾çš„ä¸‹é‡‡æ ·æ–¹å¼')
        parser.add_argument('--netF_nc', type=int, default=256)
        parser.add_argument('--nce_T', type=float, default=0.07, help='NCEæŸå¤±çš„æ¸©åº¦å‚æ•°')
        parser.add_argument('--num_patches', type=int, default=256, help='æ¯å±‚é‡‡æ ·çš„patchæ•°é‡')
        parser.add_argument('--flip_equivariance',
                          type=util.str2bool, nargs='?', const=True, default=False,
                          help="å¼ºåˆ¶ç¿»è½¬ç­‰å˜æ€§ä½œä¸ºé¢å¤–æ­£åˆ™é¡¹")

        # PTCUTç‰¹æœ‰å‚æ•°
        parser.add_argument('--lambda_cls', type=float, default=0.5, 
                          help='ç”Ÿæˆå™¨åˆ†ç±»æŸå¤±çš„æƒé‡')
        parser.add_argument('--lambda_cls_d', type=float, default=0.0, 
                          help='åˆ¤åˆ«å™¨åˆ†ç±»æŸå¤±çš„æƒé‡ï¼ˆè¾…åŠ©åˆ†ç±»å™¨GANï¼‰')
        parser.add_argument('--lambda_distill', type=float, default=0.5, 
                          help='è’¸é¦æŸå¤±çš„æƒé‡')
        parser.add_argument('--cls_temperature', type=float, default=5.0,
                          help='åˆ†ç±»æŸå¤±çš„æ¸©åº¦å‚æ•°ï¼Œç”¨äºç¼©æ”¾CONCHçš„logit_scale (é»˜è®¤5.0ï¼Œé™ä½æŸå¤±å€¼)')
        
        # CONCH å’Œ prompt features åŠ è½½å‚æ•°
        parser.add_argument('--conch_checkpoint', type=str,
                          default='/home/lzh/myCode/CONCH/checkpoints/conch/pytorch_model.bin',
                          help='CONCH é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„')
        parser.add_argument('--prompt_features_path', type=str,
                          default='/home/lzh/myCode/KgCoOp/KgCoOp/output/gnb_kgcoop_conch_csc_2class_nodular_vs_composite/prompt_text_features.pth',
                          help='KgCoOp è®­ç»ƒå¥½çš„ prompt text features è·¯å¾„')
        parser.add_argument('--num_classes', type=int, default=2, 
                          help='åˆ†ç±»æ•°é‡ï¼ˆ2åˆ†ç±»ï¼ši/nï¼‰')
        parser.add_argument('--use_labels', type=util.str2bool, nargs='?', const=True, default=True,
                          help='æ˜¯å¦ä½¿ç”¨å›¾åƒæ ‡ç­¾è¿›è¡Œåˆ†ç±»æŸå¤±è®¡ç®—ï¼ˆä»æ–‡ä»¶åæå–ï¼‰')

        parser.set_defaults(pool_size=0)
        
        # PTCUTé»˜è®¤ä½¿ç”¨ptcutæ•°æ®é›†ï¼ˆå…±äº«è£å‰ªå‚æ•°ï¼‰
        parser.set_defaults(dataset_mode='ptcut')

        opt, _ = parser.parse_known_args()

        # ä¸ºCUTå’ŒFastCUTè®¾ç½®é»˜è®¤å‚æ•°
        if opt.CUT_mode.lower() == "cut":
            parser.set_defaults(nce_idt=True, lambda_NCE=1.0)
        elif opt.CUT_mode.lower() == "fastcut":
            parser.set_defaults(
                nce_idt=False, lambda_NCE=10.0, flip_equivariance=True,
                n_epochs=150, n_epochs_decay=50
            )
        else:
            raise ValueError(opt.CUT_mode)

        return parser

    def __init__(self, opt):
        """åˆå§‹åŒ–PTCUTæ¨¡å‹
        
        æ ¸å¿ƒç»„ä»¶ï¼š
        1. CUTç»„ä»¶ï¼š
           - netG: ç”Ÿæˆå™¨ (è®­ç»ƒ)
           - netD: åˆ¤åˆ«å™¨ (è®­ç»ƒ)
           - netF: ç‰¹å¾æå–å™¨ (è®­ç»ƒ)
        
        2. CONCHç»„ä»¶ (æ–°å¢):
           - conch_model.visual: è§†è§‰ç¼–ç å™¨ (å†»ç»“,ä¸è®­ç»ƒ)
           - conch_model.logit_scale: ç›¸ä¼¼åº¦ç¼©æ”¾ (å†»ç»“)
        
        3. KgCoOpç»„ä»¶ (æ–°å¢):
           - prompt_text_features: æ–‡æœ¬ç‰¹å¾ (å†»ç»“,æ¥è‡ªKgCoOpè®­ç»ƒ)
        """
        BaseModel.__init__(self, opt)

        # æŒ‡å®šéœ€è¦æ‰“å°çš„è®­ç»ƒæŸå¤±
        self.loss_names = ['G_GAN', 'D_real', 'D_fake', 'G', 'NCE']
        
        # æ·»åŠ PTCUTç‰¹æœ‰çš„æŸå¤±
        if opt.lambda_cls > 0:
            self.loss_names.append('CLS')  # ç”Ÿæˆå™¨åˆ†ç±»æŸå¤±
            if getattr(opt, 'lambda_cls_d', 0) > 0:
                self.loss_names.append('CLS_D')  # åˆ¤åˆ«å™¨åˆ†ç±»æŸå¤±
        if opt.lambda_distill > 0:
            self.loss_names.append('DISTILL')  # è’¸é¦æŸå¤±
            
        self.visual_names = ['real_A', 'fake_B', 'real_B']
        self.nce_layers = [int(i) for i in self.opt.nce_layers.split(',')]

        if opt.nce_idt and self.isTrain:
            self.loss_names += ['NCE_Y']
            self.visual_names += ['idt_B']

        if self.isTrain:
            self.model_names = ['G', 'F', 'D']
        else:
            self.model_names = ['G']

        # ====================================================================
        # å®šä¹‰CUTç½‘ç»œï¼ˆç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨ï¼‰
        # ====================================================================
        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, 
                                     opt.normG, not opt.no_dropout, opt.init_type, 
                                     opt.init_gain, opt.no_antialias, opt.no_antialias_up, 
                                     self.gpu_ids, opt)
        self.netF = networks.define_F(opt.input_nc, opt.netF, opt.normG, not opt.no_dropout,
                                     opt.init_type, opt.init_gain, opt.no_antialias, 
                                     self.gpu_ids, opt)

        if self.isTrain:
            self.netD = networks.define_D(opt.output_nc, opt.ndf, opt.netD, opt.n_layers_D,
                                         opt.normD, opt.init_type, opt.init_gain, 
                                         opt.no_antialias, self.gpu_ids, opt)
            
            # å¦‚æœå¯ç”¨åˆ¤åˆ«å™¨åˆ†ç±»æŸå¤±ï¼Œæ·»åŠ åˆ†ç±»å¤´
            if getattr(opt, 'lambda_cls_d', 0) > 0:
                # åˆ¤åˆ«å™¨åˆ†ç±»å¤´ï¼šä»åˆ¤åˆ«å™¨ç‰¹å¾åˆ°ç±»åˆ«é¢„æµ‹
                # ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ– + å…¨è¿æ¥å±‚
                self.netD_classifier = nn.Sequential(
                    nn.AdaptiveAvgPool2d(1),  # å…¨å±€å¹³å‡æ± åŒ–
                    nn.Flatten(),
                    nn.Linear(opt.ndf * min(2 ** opt.n_layers_D, 8), opt.num_classes)
                ).to(self.device)
                self.model_names.append('D_classifier')

            # ================================================================
            # å®šä¹‰CUTæŸå¤±å‡½æ•°
            # ================================================================
            self.criterionGAN = networks.GANLoss(opt.gan_mode).to(self.device)
            self.criterionNCE = []
            for nce_layer in self.nce_layers:
                self.criterionNCE.append(PatchNCELoss(opt).to(self.device))
            self.criterionIdt = torch.nn.L1Loss().to(self.device)
            
            # ================================================================
            # å®šä¹‰PTCUTç‰¹æœ‰æŸå¤±å‡½æ•°
            # ================================================================
            if opt.lambda_cls > 0:
                # åˆ†ç±»æŸå¤±ï¼šä½¿ç”¨æ ‡å‡†äº¤å‰ç†µ
                self.criterionCLS = nn.CrossEntropyLoss().to(self.device)
            if opt.lambda_distill > 0:
                # è’¸é¦æŸå¤±ï¼šä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±
                # target=1è¡¨ç¤ºå¸Œæœ›realBå’ŒfakeBçš„ç‰¹å¾ç›¸ä¼¼
                self.criterionDistill = nn.CosineEmbeddingLoss().to(self.device)
            
            # ================================================================
            # æ­¥éª¤2: åŠ è½½ CONCH æ¨¡å‹å’Œ prompt text features
            # ================================================================
            # ç›´æ¥åŠ è½½é¢„è®­ç»ƒçš„ CONCH å’Œé¢„å…ˆä¿å­˜çš„ prompt text features
            # æ— éœ€åŠ è½½æ•´ä¸ª KgCoOp æ¨¡å‹ï¼Œæ›´ç®€æ´é«˜æ•ˆ
            print("æ­£åœ¨åŠ è½½ CONCH å’Œ prompt text features...")
            self.load_conch_and_features(opt)
            
            # ================================================================
            # å®šä¹‰ä¼˜åŒ–å™¨ï¼ˆåªä¼˜åŒ–CUTçš„ç½‘ç»œï¼‰
            # ================================================================
            # æ³¨æ„ï¼šCONCHå’ŒKgCoOpç‰¹å¾ä¸å‚ä¸ä¼˜åŒ–
            self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, 
                                              betas=(opt.beta1, opt.beta2))
            
            # åˆ¤åˆ«å™¨ä¼˜åŒ–å™¨ï¼šå¦‚æœæœ‰åˆ†ç±»å¤´ï¼Œä¸€èµ·ä¼˜åŒ–
            if getattr(opt, 'lambda_cls_d', 0) > 0:
                d_params = list(self.netD.parameters()) + list(self.netD_classifier.parameters())
                self.optimizer_D = torch.optim.Adam(d_params, lr=opt.lr, 
                                                  betas=(opt.beta1, opt.beta2))
            else:
                self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, 
                                                  betas=(opt.beta1, opt.beta2))
            
            self.optimizers.append(self.optimizer_G)
            self.optimizers.append(self.optimizer_D)

    def load_conch_and_features(self, opt):
        """
        ç›´æ¥åŠ è½½ CONCH æ¨¡å‹å’Œé¢„ä¿å­˜çš„ prompt text features
        
        ç­–ç•¥ï¼ˆæœ€ç®€åŒ–ç‰ˆï¼‰ï¼š
        1. ç›´æ¥åŠ è½½ CONCH æ¨¡å‹ï¼ˆåªéœ€è¦ visual encoderï¼‰
        2. ç›´æ¥åŠ è½½é¢„ä¿å­˜çš„ prompt_text_features.pth
        
        ä¼˜åŠ¿ï¼š
        - æ— éœ€åŠ è½½ KgCoOp çš„ text encoder å’Œ prompt learner
        - åŠ è½½é€Ÿåº¦æ›´å¿«ï¼Œå†…å­˜å ç”¨æ›´å°‘
        
        æœ€ç»ˆä¿ç•™ï¼š
        - self.image_encoder: CONCH visual encoder (86M)
        - self.logit_scale: ç›¸ä¼¼åº¦ç¼©æ”¾å‚æ•°
        - self.prompt_text_features: [num_classes, 512]
        - self.conch_preprocess: CONCH é¢„å¤„ç†æµç¨‹
        
        å‚æ•°:
            opt: å‘½ä»¤è¡Œé€‰é¡¹
        """
        print(f"\næ­£åœ¨åŠ è½½ CONCH æ¨¡å‹å’Œ prompt text features...")
        print(f"  CONCH checkpoint: {opt.conch_checkpoint}")
        print(f"  Prompt features: {opt.prompt_features_path}")
        print(f"  ç±»åˆ«æ•°: {opt.num_classes}")
        
        # ====================================================================
        # æ­¥éª¤1: åŠ è½½ CONCH æ¨¡å‹
        # ====================================================================
        print("  åŠ è½½ CONCH é¢„è®­ç»ƒæ¨¡å‹...")
        conch_model, _ = create_model_from_pretrained(
            "conch_ViT-B-16", 
            checkpoint_path=opt.conch_checkpoint
        )
        conch_model = conch_model.to(self.device)
        conch_model.eval()
        
        # æå– image encoder å’Œ logit_scale
        self.image_encoder = conch_model.visual
        self.logit_scale = conch_model.logit_scale
        
        # å†»ç»“ image encoder
        for param in self.image_encoder.parameters():
            param.requires_grad = False
        
        print(f"âœ“ CONCH image encoder å·²åŠ è½½å¹¶å†»ç»“")
        print(f"âœ“ Logit scale: {self.logit_scale.exp().item():.4f}")
        
        # ====================================================================
        # æ­¥éª¤2: åŠ è½½é¢„ä¿å­˜çš„ prompt text features
        # ====================================================================
        if not osp.exists(opt.prompt_features_path):
            raise FileNotFoundError(f"Prompt features æ–‡ä»¶ä¸å­˜åœ¨: {opt.prompt_features_path}")
        
        print(f"  åŠ è½½ prompt text features...")
        text_features = torch.load(opt.prompt_features_path, map_location=self.device, weights_only=True)
        
        # ç¡®ä¿å·²å½’ä¸€åŒ–
        text_features = text_features / text_features.norm(dim=-1, keepdim=True)
        
        # éªŒè¯å½¢çŠ¶
        if text_features.size(0) != opt.num_classes:
            raise ValueError(
                f"æ–‡æœ¬ç‰¹å¾æ•°é‡ä¸ç±»åˆ«æ•°é‡ä¸åŒ¹é…\n"
                f"  æœŸæœ›: {opt.num_classes} ä¸ªç±»åˆ«\n"
                f"  å®é™…: {text_features.size(0)} ä¸ªç‰¹å¾\n"
                f"  ç‰¹å¾å½¢çŠ¶: {text_features.shape}"
            )
        
        self.prompt_text_features = text_features
        self.prompt_text_features.requires_grad = False
        
        print(f"âœ“ Prompt text features å·²åŠ è½½ï¼Œå½¢çŠ¶: {text_features.shape}")
        
        # ====================================================================
        # æ­¥éª¤3: æ„å»º CONCH é¢„å¤„ç†æµç¨‹
        # ====================================================================
        self.conch_preprocess = build_conch_preprocess(image_size=448)
        print(f"âœ“ CONCH é¢„å¤„ç†æµç¨‹å·²æ„å»º")

        # ç¼“å­˜ mean/std ä¸ºå›ºå®šå¼ é‡ï¼Œé¿å…æ¯æ¬¡å‰å‘ä¼ æ’­é‡å¤åˆ›å»ºï¼ˆæ›¿ä»£ register_bufferï¼‰
        self.conch_mean = torch.tensor([0.48145466, 0.4578275, 0.40821073],
                                       device=self.device).view(1, 3, 1, 1)
        self.conch_std = torch.tensor([0.26862954, 0.26130258, 0.27577711],
                                      device=self.device).view(1, 3, 1, 1)

        # ====================================================================
        # æ­¥éª¤4: æ¸…ç†ä¸éœ€è¦çš„ç»„ä»¶
        # ====================================================================
        # åˆ é™¤ CONCH çš„ text encoderï¼ˆåªä¿ç•™ visualï¼‰
        del conch_model.text
        del conch_model
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print(f"âœ“ åŠ è½½å®Œæˆï¼")
        print(f"âœ“ ä¿ç•™: image_encoder (86M) + text_features ({self.prompt_text_features.numel()} å…ƒç´ )\n")



    def extract_class_labels(self, image_paths):
        """
        ä»å›¾åƒè·¯å¾„ä¸­æå–ç±»åˆ«æ ‡ç­¾
        
        æ”¯æŒä¸¤ç§æ–‡ä»¶åæ ¼å¼:
        1. æ—§æ ¼å¼: *_X.jpgï¼Œå…¶ä¸­Xæ˜¯ç±»åˆ«å·(1,2,3,4)
        2. æ–°æ ¼å¼: *_label.jpgï¼Œå…¶ä¸­labelæ˜¯å­—æ¯æ ‡ç­¾ï¼ˆå¦‚ i, nï¼‰
        
        å‚æ•°:
            image_paths: å›¾åƒè·¯å¾„åˆ—è¡¨
        è¿”å›:
            labels: ç±»åˆ«æ ‡ç­¾å¼ é‡ (batch_size,)ï¼Œç±»åˆ«ä»0å¼€å§‹ç´¢å¼•
        """
        # å®šä¹‰æ ‡ç­¾æ˜ å°„ï¼ˆå­—æ¯ -> æ•°å­—ç´¢å¼•ï¼‰
        label_map = {
            'i': 0,  # intermixed/composite
            'n': 1,  # nodular
            # å…¼å®¹4ç±»æƒ…å†µ
            '1': 0, '2': 1, '3': 2, '4': 3
        }
        
        labels = []
        for path in image_paths:
            # æå–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            filename = osp.splitext(osp.basename(path))[0]
            
            # å°è¯•æå–æ ‡ç­¾ï¼ˆæœ€åä¸€ä¸ªä¸‹åˆ’çº¿åçš„éƒ¨åˆ†ï¼‰
            try:
                # åˆ†å‰²æ–‡ä»¶åï¼Œè·å–æœ€åä¸€éƒ¨åˆ†
                parts = filename.split('_')
                label_str = parts[-1]  # æœ€åä¸€éƒ¨åˆ†åº”è¯¥æ˜¯æ ‡ç­¾
                
                if label_str in label_map:
                    label = label_map[label_str]
                elif label_str.isdigit():
                    # å¦‚æœæ˜¯æ•°å­—ï¼Œè½¬æ¢ä¸º0-indexed
                    label = int(label_str) - 1
                else:
                    raise ValueError(f"æœªçŸ¥æ ‡ç­¾: {label_str}")
                    
            except Exception as e:
                print(f"è­¦å‘Š: æ— æ³•ä» {path} æå–ç±»åˆ«æ ‡ç­¾ ({e})ï¼Œä½¿ç”¨é»˜è®¤å€¼0")
                label = 0
                
            labels.append(label)
        
        return torch.tensor(labels, dtype=torch.long, device=self.device)

    def data_dependent_initialize(self, data):
        """
        ç‰¹å¾ç½‘ç»œnetFçš„å®šä¹‰ä¾èµ–äºnetGç¼–ç å™¨éƒ¨åˆ†ä¸­é—´ç‰¹å¾çš„å½¢çŠ¶
        """
        import gc

        bs_per_gpu = data["A"].size(0) // max(len(self.opt.gpu_ids), 1)
        self.set_input(data)
        self.real_A = self.real_A[:bs_per_gpu]
        self.real_B = self.real_B[:bs_per_gpu]
        
        print(f"  [DEBUG] real_A shape: {self.real_A.shape}, real_B shape: {self.real_B.shape}")
        
        self.forward()
        
        if self.opt.isTrain:
            self.compute_D_loss().backward()
            
            self.compute_G_loss().backward()
            
            if self.opt.lambda_NCE > 0.0:
                self.optimizer_F = torch.optim.Adam(self.netF.parameters(), lr=self.opt.lr,
                                                   betas=(self.opt.beta1, self.opt.beta2))
                self.optimizers.append(self.optimizer_F)

    def optimize_parameters(self):
        """ä¼˜åŒ–å‚æ•°ï¼šæ›´æ–°åˆ¤åˆ«å™¨Då’Œç”Ÿæˆå™¨G"""
        # å‰å‘ä¼ æ’­
        self.forward()

        # æ›´æ–°åˆ¤åˆ«å™¨D
        self.set_requires_grad(self.netD, True)
        self.optimizer_D.zero_grad()
        self.loss_D = self.compute_D_loss()
        self.loss_D.backward()
        self.optimizer_D.step()

        # æ›´æ–°ç”Ÿæˆå™¨G
        self.set_requires_grad(self.netD, False)
        self.optimizer_G.zero_grad()
        if self.opt.netF == 'mlp_sample':
            self.optimizer_F.zero_grad()
        self.loss_G = self.compute_G_loss()
        self.loss_G.backward()
        self.optimizer_G.step()
        if self.opt.netF == 'mlp_sample':
            self.optimizer_F.step()

    def set_input(self, input):
        """ä»dataloaderä¸­è§£åŒ…è¾“å…¥æ•°æ®å¹¶è¿›è¡Œå¿…è¦çš„é¢„å¤„ç†"""
        AtoB = self.opt.direction == 'AtoB'
        self.real_A = input['A' if AtoB else 'B'].to(self.device)
        self.real_B = input['B' if AtoB else 'A'].to(self.device)
        self.image_paths = input['A_paths' if AtoB else 'B_paths']
        
        # æå–ç±»åˆ«æ ‡ç­¾ï¼ˆå¦‚æœä½¿ç”¨åˆ†ç±»æŸå¤±ï¼‰
        # æ³¨æ„ï¼šå¿…é¡»åœ¨è®­ç»ƒæ—¶ä¸”å¯ç”¨æ ‡ç­¾æ—¶æ‰æå–
        if self.isTrain and self.opt.lambda_cls > 0 and self.opt.use_labels:
            self.labels = self.extract_class_labels(self.image_paths)
        elif self.isTrain and self.opt.lambda_cls > 0 and not self.opt.use_labels:
            # å¦‚æœå¯ç”¨äº† CLS æŸå¤±ä½†æ²¡æœ‰å¯ç”¨æ ‡ç­¾ï¼Œå‘å‡ºè­¦å‘Š
            if not hasattr(self, '_label_warning_printed'):
                print("\nâš ï¸  è­¦å‘Š: lambda_cls > 0 ä½† use_labels=False")
                print("  CLS æŸå¤±å°†æ— æ³•æä¾›æœ‰æ•ˆç›‘ç£")
                print("  å»ºè®®è®¾ç½® --use_labels True ä»¥å¯ç”¨æ ‡ç­¾ç›‘ç£\n")
                self._label_warning_printed = True

    def forward(self):
        """å‰å‘ä¼ æ’­ï¼›è¢«<optimize_parameters>å’Œ<test>è°ƒç”¨"""
        self.real = torch.cat((self.real_A, self.real_B), dim=0) if self.opt.nce_idt and self.opt.isTrain else self.real_A
        if self.opt.flip_equivariance:
            self.flipped_for_equivariance = self.opt.isTrain and (np.random.random() < 0.5)
            if self.flipped_for_equivariance:
                self.real = torch.flip(self.real, [3])

        # è°ƒè¯•ï¼šæ‰“å°è¾“å…¥å°ºå¯¸
        if not hasattr(self, '_forward_debug_printed'):
            print(f"  [DEBUG] forward: self.real shape = {self.real.shape}")
            self._forward_debug_printed = True
        
        self.fake = self.netG(self.real)
        self.fake_B = self.fake[:self.real_A.size(0)]
        if self.opt.nce_idt:
            self.idt_B = self.fake[self.real_A.size(0):]

    def compute_D_loss(self):
        """è®¡ç®—åˆ¤åˆ«å™¨çš„GANæŸå¤± + åˆ†ç±»æŸå¤±ï¼ˆå¯é€‰ï¼‰"""
        fake = self.fake_B.detach()
        pred_fake = self.netD(fake)
        self.loss_D_fake = self.criterionGAN(pred_fake, False).mean()
        
        self.pred_real = self.netD(self.real_B)
        loss_D_real = self.criterionGAN(self.pred_real, True)
        self.loss_D_real = loss_D_real.mean()

        # åŸºç¡€GANæŸå¤±
        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5
        
        # æ·»åŠ åˆ¤åˆ«å™¨åˆ†ç±»æŸå¤±ï¼ˆè¾…åŠ©åˆ†ç±»å™¨GAN, AC-GANï¼‰
        if getattr(self.opt, 'lambda_cls_d', 0) > 0 and self.opt.use_labels:
            # ä»çœŸå®å›¾åƒçš„åˆ¤åˆ«å™¨ç‰¹å¾ä¸­æå–åˆ†ç±»logits
            # éœ€è¦è·å–åˆ¤åˆ«å™¨çš„ä¸­é—´ç‰¹å¾
            d_features_real = self.get_D_features(self.real_B)
            cls_logits_real = self.netD_classifier(d_features_real)
            
            # ä½¿ç”¨çœŸå®æ ‡ç­¾è®¡ç®—åˆ†ç±»æŸå¤±
            if hasattr(self, 'labels'):
                self.loss_CLS_D = self.criterionCLS(cls_logits_real, self.labels) * self.opt.lambda_cls_d
                self.loss_D = self.loss_D + self.loss_CLS_D
                
                # è°ƒè¯•ä¿¡æ¯ï¼ˆä»…é¦–æ¬¡ï¼‰
                if not hasattr(self, '_cls_d_debug_printed'):
                    with torch.no_grad():
                        pred_labels = cls_logits_real.argmax(dim=1)
                        accuracy = (pred_labels == self.labels).float().mean().item()
                    print(f"\n[Discriminator CLS Loss Debug]")
                    print(f"  D features shape: {d_features_real.shape}")
                    print(f"  D logits shape: {cls_logits_real.shape}")
                    print(f"  D Batch accuracy: {accuracy:.2%}")
                    print(f"  D CLS Loss value: {self.loss_CLS_D.item():.4f}\n")
                    self._cls_d_debug_printed = True
        
        return self.loss_D
    
    def get_D_features(self, x):
        """ä»åˆ¤åˆ«å™¨ä¸­æå–ä¸­é—´ç‰¹å¾ç”¨äºåˆ†ç±»"""
        # éå†åˆ¤åˆ«å™¨çš„å±‚ï¼Œæå–å€’æ•°ç¬¬äºŒå±‚ï¼ˆæœ€åçš„LeakyReLUï¼‰çš„è¾“å‡º
        # å€’æ•°ç¬¬äºŒå±‚è¾“å‡ºç»´åº¦åº”è¯¥æ˜¯ [batch, ndf*8, H, W]ï¼Œå³ [batch, 512, H, W]
        
        # å¤„ç†DataParallelåŒ…è£…çš„æƒ…å†µ
        netD = self.netD.module if hasattr(self.netD, 'module') else self.netD
        
        if hasattr(netD, 'model'):
            # NLayerDiscriminatorä½¿ç”¨Sequential
            # model[-1] æ˜¯æœ€åçš„ Conv2d(512, 1, 4, 1, 1) è¾“å‡ºçœŸå‡é¢„æµ‹
            # model[:-1] æ˜¯å‰é¢æ‰€æœ‰å±‚ï¼Œæœ€åä¸€å±‚æ˜¯ LeakyReLU
            features = x
            # æ’é™¤æœ€åä¸€å±‚ï¼ˆ1x1å·ç§¯å±‚ï¼‰ï¼Œä¿ç•™åˆ°LeakyReLU
            for i, layer in enumerate(netD.model[:-1]):
                features = layer(features)
            # ç°åœ¨ features åº”è¯¥æ˜¯ [batch, 512, H, W]
            return features
        else:
            # å…¶ä»–åˆ¤åˆ«å™¨ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨è¾“å…¥ï¼ˆå¯èƒ½éœ€è¦æ ¹æ®å…·ä½“ç±»å‹è°ƒæ•´ï¼‰
            raise NotImplementedError("å½“å‰åªæ”¯æŒNLayerDiscriminatorçš„åˆ†ç±»å¤´")

    def compute_G_loss(self):
        """è®¡ç®—ç”Ÿæˆå™¨çš„æŸå¤±ï¼šGAN + NCE + CLS + DISTILL"""
        fake = self.fake_B

        # 1. GANæŸå¤±
        if self.opt.lambda_GAN > 0.0:
            pred_fake = self.netD(fake)
            self.loss_G_GAN = self.criterionGAN(pred_fake, True).mean() * self.opt.lambda_GAN
        else:
            self.loss_G_GAN = 0.0

        # 2. NCEæŸå¤±
        if self.opt.lambda_NCE > 0.0:
            self.loss_NCE = self.calculate_NCE_loss(self.real_A, self.fake_B)
            if self.opt.nce_idt:
                self.loss_NCE_Y = self.calculate_NCE_loss(self.real_B, self.idt_B)
                loss_NCE_both = (self.loss_NCE + self.loss_NCE_Y) * 0.5
            else:
                loss_NCE_both = self.loss_NCE
        else:
            loss_NCE_both = 0.0

        # ====================================================================
        # ğŸš€ æé€Ÿä¼˜åŒ–åŒº: ç»Ÿä¸€æå– CONCH ç‰¹å¾ï¼Œé¿å…é‡å¤è®¡ç®—
        # ====================================================================
        if self.opt.lambda_cls > 0.0 or self.opt.lambda_distill > 0.0:
            _, _, h, w = self.fake_B.shape
            # ä»…ç”Ÿæˆä¸€æ¬¡åŒä½åæ ‡ï¼ŒCLS å’Œ DISTILL å…±äº«åŒä¸€è£å‰ªåŒºåŸŸ
            i, j = self.get_random_crop_coords(h, w, crop_size=448)

            fake_B_conch_in = self.differentiable_conch_preprocess(self.fake_B, i, j, crop_size=448)
            real_B_conch_in = self.differentiable_conch_preprocess(self.real_B, i, j, crop_size=448)

            # ä½¿ç”¨ AMP åŠ é€Ÿ ViT çš„çŸ©é˜µä¹˜æ³•ï¼ˆTensor Coreï¼‰ï¼Œæ˜¾è‘—æå‡é€Ÿåº¦å¹¶èŠ‚çœæ˜¾å­˜
            from torch.amp import autocast
            with autocast('cuda'):
                # fake_B ç‰¹å¾ï¼šå¿…é¡»å…è®¸æ¢¯åº¦å›ä¼ åˆ°ç”Ÿæˆå™¨
                fake_B_features = self.image_encoder(fake_B_conch_in)
                if isinstance(fake_B_features, tuple):
                    fake_B_features = fake_B_features[0]
                fake_B_features = fake_B_features / fake_B_features.norm(dim=-1, keepdim=True)

                # real_B ç‰¹å¾ï¼šçœŸå®å›¾åƒï¼Œä¸éœ€è¦æ±‚å¯¼
                with torch.no_grad():
                    real_B_features = self.image_encoder(real_B_conch_in)
                    if isinstance(real_B_features, tuple):
                        real_B_features = real_B_features[0]
                    real_B_features = real_B_features / real_B_features.norm(dim=-1, keepdim=True)

            # è½¬å› FP32 ä¿è¯æŸå¤±è®¡ç®—çš„æ•°å€¼ç¨³å®šæ€§
            self.fake_B_conch_feat = fake_B_features.float()
            self.real_B_conch_feat = real_B_features.float()

        # 3. åˆ†ç±»æŸå¤± (ç›´æ¥ä¼ å…¥é¢„è®¡ç®—å¥½çš„ç‰¹å¾ï¼Œæ— é¢å¤–æ¨ç†å¼€é”€)
        if self.opt.lambda_cls > 0.0:
            self.loss_CLS = self.compute_classification_loss(self.fake_B_conch_feat)
        else:
            self.loss_CLS = 0.0

        # 4. è’¸é¦æŸå¤± (ç›´æ¥ä¼ å…¥é¢„è®¡ç®—å¥½çš„ç‰¹å¾ï¼Œæ— é¢å¤–æ¨ç†å¼€é”€)
        if self.opt.lambda_distill > 0.0:
            self.loss_DISTILL = self.compute_distillation_loss(self.fake_B_conch_feat, self.real_B_conch_feat)
        else:
            self.loss_DISTILL = 0.0

        # æ€»ç”Ÿæˆå™¨æŸå¤±
        self.loss_G = self.loss_G_GAN + loss_NCE_both + self.loss_CLS + self.loss_DISTILL
        return self.loss_G


    def get_random_crop_coords(self, h, w, crop_size=448):
        """ç”Ÿæˆéšæœºè£å‰ªçš„åæ ‡ï¼Œç”¨äºä»é«˜åˆ†è¾¨ç‡å›¾åƒä¸­æå– patch"""
        if h <= crop_size or w <= crop_size:
            return 0, 0
        i = random.randint(0, h - crop_size)
        j = random.randint(0, w - crop_size)
        return i, j

    def differentiable_conch_preprocess(self, image_tensor, i, j, crop_size=448):
        """å…¨è¿‡ç¨‹å¯å¯¼çš„é¢„å¤„ç† (æé€Ÿç‰ˆï¼šä½¿ç”¨ç¼“å­˜çš„ mean/std buffer)"""
        # 1. è¿˜åŸåˆ° [0, 1]
        img = (image_tensor + 1.0) / 2.0
        # 2. ç©ºé—´è£å‰ª (åˆ‡ç‰‡æ“ä½œå®Œå…¨ä¿ç•™æ¢¯åº¦)
        img_crop = img[:, :, i:i+crop_size, j:j+crop_size]
        # 3. æ ‡å‡†åŒ– (ç›´æ¥ä½¿ç”¨ç¼“å­˜çš„å¼ é‡ï¼Œé¿å…é‡æ–°å¼€è¾Ÿæ˜¾å­˜)
        img_norm = (img_crop - self.conch_mean) / self.conch_std
        return img_norm

    def compute_classification_loss(self, fake_B_features):
        """è®¡ç®—åˆ†ç±»æŸå¤± (æé€Ÿç‰ˆï¼šç›´æ¥ä½¿ç”¨é¢„å…ˆè®¡ç®—å¥½çš„ç‰¹å¾)"""
        if not self.opt.use_labels or not hasattr(self, 'labels'):
            return torch.tensor(0.0, device=self.device, requires_grad=True)

        logit_scale = self.logit_scale.exp()
        temperature = getattr(self.opt, 'cls_temperature', 1.0)
        logits = (logit_scale / temperature) * fake_B_features @ self.prompt_text_features.t()

        loss_cls = self.criterionCLS(logits, self.labels) * self.opt.lambda_cls
        return loss_cls

    def compute_distillation_loss(self, fake_B_features, real_B_features):
        """è®¡ç®—è’¸é¦æŸå¤± (æé€Ÿç‰ˆï¼šç›´æ¥ä½¿ç”¨é¢„å…ˆè®¡ç®—å¥½çš„ç‰¹å¾)"""
        target = torch.ones(real_B_features.size(0), device=self.device)
        loss_distill = self.criterionDistill(
            fake_B_features,
            real_B_features,
            target
        ) * self.opt.lambda_distill
        return loss_distill

    def calculate_NCE_loss(self, src, tgt):
        """è®¡ç®—NCEå¯¹æ¯”æŸå¤±"""
        n_layers = len(self.nce_layers)
        feat_q = self.netG(tgt, self.nce_layers, encode_only=True)

        if self.opt.flip_equivariance and self.flipped_for_equivariance:
            feat_q = [torch.flip(fq, [3]) for fq in feat_q]

        feat_k = self.netG(src, self.nce_layers, encode_only=True)
        feat_k_pool, sample_ids = self.netF(feat_k, self.opt.num_patches, None)
        feat_q_pool, _ = self.netF(feat_q, self.opt.num_patches, sample_ids)

        total_nce_loss = 0.0
        for f_q, f_k, crit, nce_layer in zip(feat_q_pool, feat_k_pool, self.criterionNCE, self.nce_layers):
            loss = crit(f_q, f_k) * self.opt.lambda_NCE
            total_nce_loss += loss.mean()

        return total_nce_loss / n_layers
