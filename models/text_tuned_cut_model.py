import torch
import torch.nn.functional as F
from .cut_model import CUTModel
from util.text_generator import TextGenerator
from conch.open_clip_custom import create_model_from_pretrained, get_tokenizer, tokenize


class TextTunedCUTModel(CUTModel):
    """
    æ–‡æœ¬è°ƒä¼˜çš„CUTæ¨¡å‹ï¼Œç”¨äºè™šæ‹ŸæŸ“è‰²ä»»åŠ¡
    åœ¨CUTæ¨¡å‹åŸºç¡€ä¸Šå¢åŠ æ–‡æœ¬è°ƒä¼˜åŠŸèƒ½ï¼ŒåŒ…å«ï¼š
    1. ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆç—…ç†ç»„ç»‡ç»†èƒHEå›¾åƒæè¿°
    2. ä½¿ç”¨CONCHæ¨¡å‹æå–å›¾åƒå’Œæ–‡æœ¬çš„embedding
    3. è®¡ç®—æ–‡æœ¬-å›¾åƒç›¸ä¼¼åº¦æŸå¤±(loss1)å’Œå›¾åƒembeddingæŸå¤±(loss2)
    
    å¤šGPUè®­ç»ƒä¼˜åŒ–ç‰¹æ€§ï¼š
    - CONCHæ¨¡å‹æ”¯æŒDataParallelå¹¶è¡Œæ¨ç†
    - æ–‡æœ¬embeddingsé¢„è®¡ç®—å¹¶ç¼“å­˜åœ¨CPUï¼ŒæŒ‰éœ€ç§»åˆ°GPU
    - æ‰¹é‡å¤„ç†å›¾åƒç¼–ç ï¼Œå‡å°‘GPUé—´æ•°æ®ä¼ è¾“
    - æ™ºèƒ½è®¾å¤‡ç®¡ç†ï¼Œç¡®ä¿æ‰€æœ‰tensoråœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    - å†…å­˜ä¼˜åŒ–ï¼Œæ”¯æŒå¤§batch_sizeè®­ç»ƒ
    """

    @staticmethod
    def modify_commandline_options(parser, is_train=True):
        """ é…ç½®æ–‡æœ¬è°ƒä¼˜CUTæ¨¡å‹ç‰¹æœ‰çš„å‚æ•°é€‰é¡¹ """
        parser = CUTModel.modify_commandline_options(parser, is_train)
        
        # æ–‡æœ¬è°ƒä¼˜ç›¸å…³å‚æ•°
        parser.add_argument('--lambda_text', type=float, default=1, help='æ–‡æœ¬ç›¸ä¼¼åº¦æŸå¤±çš„æƒé‡')
        parser.add_argument('--lambda_feat', type=float, default=1, help='ç‰¹å¾ç›¸ä¼¼åº¦æŸå¤±çš„æƒé‡')
        # æµ‹è¯•æ¨¡å¼ä¸‹ä¸éœ€è¦æ–‡æœ¬æè¿°æ–‡ä»¶
        parser.add_argument('--text_descriptions_file', type=str, 
                           required=is_train,  # ä»…è®­ç»ƒæ—¶å¿…éœ€
                           default='',
                           help='ç—…ç†ç»„ç»‡HEå›¾åƒæ–‡æœ¬æè¿°æ–‡ä»¶è·¯å¾„ï¼ˆè®­ç»ƒæ—¶å¿…éœ€ï¼‰')
        return parser

    def __init__(self, opt):
        super(TextTunedCUTModel, self).__init__(opt)
        
        # æ ‡è®°æ¨¡å‹å°šæœªå®Œå…¨åˆå§‹åŒ–
        self._initialized = False
        
        # ä¿å­˜é€‰é¡¹
        self.opt = opt
        
        # æ£€æŸ¥æ˜¯å¦åœ¨è®­ç»ƒæ¨¡å¼
        self.isTrain = opt.isTrain
        
        # æµ‹è¯•æ¨¡å¼ä¼˜åŒ–ï¼šè·³è¿‡CONCHå’Œæ–‡æœ¬ç›¸å…³çš„åˆå§‹åŒ–
        if not self.isTrain:
            print("ğŸš€ æµ‹è¯•æ¨¡å¼ï¼šè·³è¿‡CONCHæ¨¡å‹å’Œæ–‡æœ¬æè¿°åŠ è½½ï¼ŒèŠ‚çœå†…å­˜...")
            self.conch_model = None
            self.conch_device = None
            self.text_generator = None
            self.text_embeddings = None
            self.text_descriptions = []
            # æµ‹è¯•æ¨¡å¼ä¸éœ€è¦è¿™äº›æŸå¤±
            return
            
        # ============ ä»¥ä¸‹ä»…åœ¨è®­ç»ƒæ¨¡å¼æ‰§è¡Œ ============
        print("ğŸ”§ è®­ç»ƒæ¨¡å¼ï¼šåˆå§‹åŒ–CONCHæ¨¡å‹å’Œæ–‡æœ¬æè¿°...")
        
        # åˆå§‹åŒ–CONCHæ¨¡å‹åŒ…è£…å™¨ï¼ˆåœ¨ä¸»GPUä¸Šï¼‰
        # è·å–ä¸»è®¾å¤‡ï¼ˆç¬¬ä¸€ä¸ªGPUæˆ–CPUï¼‰
        self.conch_device = torch.device(f'cuda:{opt.gpu_ids[0]}' if len(opt.gpu_ids) > 0 else 'cpu')
        
        self.conch_model, self.conch_preprocess = create_model_from_pretrained(
            "conch_ViT-B-16", 
            checkpoint_path="checkpoints/conch/pytorch_model.bin"
            )
        self.conch_model.eval()
        self.conch_model.to(self.conch_device)
        
        # å¤šGPUä¼˜åŒ–ï¼šå¦‚æœæœ‰å¤šä¸ªGPUï¼Œä½¿ç”¨DataParallelåŒ…è£…CONCHæ¨¡å‹
        if len(opt.gpu_ids) > 1:
            self.conch_model = torch.nn.DataParallel(self.conch_model, device_ids=opt.gpu_ids)
            print(f"CONCHæ¨¡å‹å·²å¯ç”¨DataParallelï¼Œä½¿ç”¨GPU: {opt.gpu_ids}")
        
        # åˆå§‹åŒ–æ–‡æœ¬ç”Ÿæˆå™¨
        if not hasattr(opt, 'text_descriptions_file') or not opt.text_descriptions_file:
            raise ValueError("å¿…é¡»æŒ‡å®šæ–‡æœ¬æè¿°æ–‡ä»¶è·¯å¾„ --text_descriptions_file")
            
        self.text_generator = TextGenerator(text_file_path=opt.text_descriptions_file)
            
        # æ·»åŠ æ–°çš„æŸå¤±åç§°
        self.loss_names.extend(['TEXT', 'FEAT'])
        
        # æ–‡æœ¬æè¿°ç¼“å­˜
        self.text_descriptions = []
        self.text_embeddings = None  # åˆå§‹åŒ–ä¸ºNoneï¼Œç¨ååŠ è½½
        
        # ä¸€æ¬¡æ€§åŠ è½½å¹¶ç¼–ç æ‰€æœ‰æ–‡æœ¬æè¿°
        self._load_and_encode_all_texts()

    def _load_and_encode_all_texts(self):
        """
        ä¸€æ¬¡æ€§åŠ è½½å¹¶ç¼–ç æ‰€æœ‰æ–‡æœ¬æè¿°ï¼ˆæœ€å¤§åŒ–æ€§èƒ½ä¼˜åŒ–ï¼‰
        å¤šGPUä¼˜åŒ–ï¼šæ–‡æœ¬embeddingsåªåœ¨ä¸»GPUä¸Šè®¡ç®—ä¸€æ¬¡ï¼Œç„¶åå›ºå®šä½
        """
        # åŠ è½½æ–‡ä»¶ä¸­çš„æ‰€æœ‰æè¿°
        all_descriptions = self.text_generator.get_all_descriptions()
        print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(all_descriptions)} ä¸ªæ–‡æœ¬æè¿°")
        
        # ä¸€æ¬¡æ€§ç¼–ç æ‰€æœ‰æè¿°ï¼Œè®­ç»ƒä¸­ä¸å†é‡å¤è®¡ç®—
        tokenizer = get_tokenizer()
        text_emb_list = []
        
        # æ‰¹é‡å¤„ç†æ–‡æœ¬ä»¥æé«˜æ•ˆç‡
        batch_size = 16  # å¯ä»¥æ ¹æ®GPUå†…å­˜è°ƒæ•´
        for i in range(0, len(all_descriptions), batch_size):
            batch_texts = all_descriptions[i:i+batch_size]
            text_tokens = tokenize(texts=batch_texts, tokenizer=tokenizer).to(self.conch_device)
            
            with torch.inference_mode():
                # å¤„ç†DataParallelåŒ…è£…çš„æ¨¡å‹
                if isinstance(self.conch_model, torch.nn.DataParallel):
                    text_emb = self.conch_model.module.encode_text(text_tokens, normalize=True)
                else:
                    text_emb = self.conch_model.encode_text(text_tokens, normalize=True)
            
            # ç§»åˆ°CPUä»¥èŠ‚çœGPUå†…å­˜ï¼Œè®­ç»ƒæ—¶å†ç§»åˆ°å¯¹åº”è®¾å¤‡
            text_emb_list.append(text_emb.cpu())
        
        # stack åå½¢çŠ¶: [N, 512]ï¼Œå­˜å‚¨åœ¨CPUä¸Š
        self.text_embeddings = torch.cat(text_emb_list, dim=0)
        print(f"æ–‡æœ¬ç¼–ç å®Œæˆï¼Œå½¢çŠ¶: {self.text_embeddings.shape}ï¼Œå­˜å‚¨åœ¨CPUï¼Œè®­ç»ƒä¸­å°†æŒ‰éœ€ç§»åˆ°GPU")

    def compute_text_similarity_loss(self, generated_images, text_embeddings):
        """
        è®¡ç®—ç”Ÿæˆå›¾åƒä¸æ–‡æœ¬æè¿°çš„ç›¸ä¼¼åº¦æŸå¤± (loss1)
        é‡‡ç”¨å¯¹æ¯”å­¦ä¹ æ€æƒ³ï¼Œä½¿ç”Ÿæˆå›¾åƒçš„embeddingä¸æ–‡æœ¬æè¿°çš„embeddingæ›´æ¥è¿‘
        å¤šGPUä¼˜åŒ–ï¼šç¡®ä¿æ‰€æœ‰tensoråœ¨åŒä¸€è®¾å¤‡ä¸Š
        """
        # æµ‹è¯•æ¨¡å¼æˆ–æ— æ•ˆè¾“å…¥æ—¶è¿”å›é›¶æŸå¤±
        if not self.isTrain or self.conch_model is None:
            return torch.tensor(0.0, requires_grad=True, device=generated_images.device)
        
        if text_embeddings is None or text_embeddings.shape[0] == 0:
            return torch.tensor(0.0, requires_grad=True, device=generated_images.device)

        # è·å–å½“å‰batchçš„è®¾å¤‡
        current_device = generated_images.device
        
        # å°†æ–‡æœ¬embeddingsç§»åˆ°å½“å‰è®¾å¤‡ï¼ˆå¦‚æœè¿˜åœ¨CPUä¸Šï¼‰
        text_embeddings = text_embeddings.to(current_device)

        # é¢„å¤„ç†ç”Ÿæˆå›¾åƒï¼šTensor (B, C, H, W) å€¼åŸŸ[-1,1] -> è°ƒæ•´å¤§å°å¹¶å½’ä¸€åŒ–
        # 1. å°†å€¼åŸŸä» [-1, 1] è½¬æ¢åˆ° [0, 1]
        imgs = (generated_images + 1) / 2.0
        # 2. è°ƒæ•´åˆ° CONCH æœŸæœ›çš„è¾“å…¥å°ºå¯¸ (512x512)
        imgs = F.interpolate(imgs, size=(512, 512), mode='bilinear', align_corners=False)
        # 3. å½’ä¸€åŒ–ï¼ˆCONCH ä½¿ç”¨ ImageNet ç»Ÿè®¡å€¼ï¼‰
        mean = torch.tensor([0.485, 0.456, 0.406], device=current_device).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225], device=current_device).view(1, 3, 1, 1)
        image_tensor = (imgs - mean) / std

        # ç¼–ç ç”Ÿæˆå›¾åƒï¼ˆä½¿ç”¨inference_modeè€Œéno_gradä»¥æé«˜æ€§èƒ½ï¼‰
        with torch.inference_mode():
            # å°†å›¾åƒç§»åˆ°CONCHè®¾å¤‡è¿›è¡Œæ¨ç†
            image_tensor_conch = image_tensor.to(self.conch_device)
            
            # å¤„ç†DataParallelåŒ…è£…çš„æ¨¡å‹
            if isinstance(self.conch_model, torch.nn.DataParallel):
                gen_img_embeddings = self.conch_model.module.encode_image(image_tensor_conch, normalize=True)
            else:
                gen_img_embeddings = self.conch_model.encode_image(image_tensor_conch, normalize=True)
            
            # ç§»å›å½“å‰è®¾å¤‡
            gen_img_embeddings = gen_img_embeddings.to(current_device)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ (B, N)
        # gen_img_embeddings: [B, 512], text_embeddings: [N, 512]
        # ç»“æœ: [B, N]
        similarity = torch.matmul(
            F.normalize(gen_img_embeddings, dim=1),
            F.normalize(text_embeddings, dim=1).transpose(0, 1)  # [N, 512] -> [512, N]
        )

        # æŸå¤±ï¼šæ¯ä¸ªç”Ÿæˆå›¾åƒä¸æ‰€æœ‰æ–‡æœ¬æè¿°çš„æœ€å¤§ç›¸ä¼¼åº¦
        max_similarities, _ = similarity.max(dim=1)
        loss = 1 - max_similarities.mean()

        return loss

    def compute_feature_similarity_loss(self, real_images, generated_images):
        """
        è®¡ç®—çœŸå®å›¾åƒä¸ç”Ÿæˆå›¾åƒçš„embeddingæŸå¤± (loss2)
        ä½¿ç”Ÿæˆå›¾åƒçš„ç‰¹å¾ä¸çœŸå®å›¾åƒçš„ç‰¹å¾ä¿æŒä¸€è‡´
        å¤šGPUä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†å›¾åƒç¼–ç ï¼Œå‡å°‘è®¾å¤‡é—´ä¼ è¾“
        """
        # æµ‹è¯•æ¨¡å¼æ—¶è¿”å›é›¶æŸå¤±
        if not self.isTrain or self.conch_model is None:
            return torch.tensor(0.0, requires_grad=True, device=generated_images.device)
        
        # è·å–å½“å‰batchçš„è®¾å¤‡
        current_device = generated_images.device
        
        # é¢„å¤„ç†å‡½æ•°ï¼šTensoré¢„å¤„ç†
        def preprocess_tensor(imgs):
            # 1. å°†å€¼åŸŸä» [-1, 1] è½¬æ¢åˆ° [0, 1]
            imgs = (imgs + 1) / 2.0
            # 2. è°ƒæ•´åˆ° CONCH æœŸæœ›çš„è¾“å…¥å°ºå¯¸ (512x512)
            imgs = F.interpolate(imgs, size=(512, 512), mode='bilinear', align_corners=False)
            # 3. å½’ä¸€åŒ–ï¼ˆCONCH ä½¿ç”¨ ImageNet ç»Ÿè®¡å€¼ï¼‰
            mean = torch.tensor([0.485, 0.456, 0.406], device=current_device).view(1, 3, 1, 1)
            std = torch.tensor([0.229, 0.224, 0.225], device=current_device).view(1, 3, 1, 1)
            return (imgs - mean) / std
        
        real_images_tensor = preprocess_tensor(real_images)
        generated_images_tensor = preprocess_tensor(generated_images)
        
        # æ‰¹é‡ç¼–ç ä»¥å‡å°‘å¼€é”€
        with torch.inference_mode():
            # åˆå¹¶çœŸå®å›¾åƒå’Œç”Ÿæˆå›¾åƒè¿›è¡Œæ‰¹é‡å¤„ç†
            combined_images = torch.cat([real_images_tensor, generated_images_tensor], dim=0)
            combined_images = combined_images.to(self.conch_device)
            
            # å¤„ç†DataParallelåŒ…è£…çš„æ¨¡å‹
            if isinstance(self.conch_model, torch.nn.DataParallel):
                combined_embeddings = self.conch_model.module.encode_image(combined_images, normalize=True)
            else:
                combined_embeddings = self.conch_model.encode_image(combined_images, normalize=True)
            
            # ç§»å›å½“å‰è®¾å¤‡å¹¶åˆ†ç¦»çœŸå®å’Œç”Ÿæˆå›¾åƒçš„embeddings
            combined_embeddings = combined_embeddings.to(current_device)
            batch_size = real_images.shape[0]
            real_img_embeddings = combined_embeddings[:batch_size]
            gen_img_embeddings = combined_embeddings[batch_size:]

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦æŸå¤±
        similarity = torch.cosine_similarity(real_img_embeddings, gen_img_embeddings, dim=1)
        loss = 1 - similarity.mean()

        return loss

    def data_dependent_initialize(self, data):
        """é‡å†™çˆ¶ç±»æ–¹æ³•ï¼Œåœ¨åˆå§‹åŒ–å®Œæˆåæ ‡è®°"""
        super().data_dependent_initialize(data)
        self._initialized = True
    
    def forward(self):
        """å‰å‘ä¼ æ’­ï¼›è¢«<optimize_parameters>å’Œ<test>è°ƒç”¨ã€‚"""
        # è°ƒç”¨çˆ¶ç±»çš„å‰å‘ä¼ æ’­
        super().forward()

    def compute_G_loss(self):
        """è®¡ç®—ç”Ÿæˆå™¨çš„GANã€NCEå’Œæ–‡æœ¬è°ƒä¼˜æŸå¤±"""
        # è°ƒç”¨çˆ¶ç±»è®¡ç®—åŸå§‹æŸå¤±
        loss_G = super().compute_G_loss()
        
        # æµ‹è¯•æ¨¡å¼ï¼šåªè¿”å›çˆ¶ç±»æŸå¤±ï¼Œä¸è®¡ç®—æ–‡æœ¬è°ƒä¼˜æŸå¤±
        if not self.isTrain:
            self.loss_G = loss_G
            return self.loss_G
        
        # åœ¨åˆå§‹åŒ–é˜¶æ®µè·³è¿‡æ–‡æœ¬è°ƒä¼˜æŸå¤±ï¼Œé¿å…å¡ä½
        if not hasattr(self, '_initialized') or not self._initialized:
            self.loss_TEXT = torch.tensor(0.0, requires_grad=True).to(self.device)
            self.loss_FEAT = torch.tensor(0.0, requires_grad=True).to(self.device)
            self.loss_G = loss_G
            return self.loss_G
        
        # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦æŸå¤±
        if getattr(self.opt, 'lambda_text', 0.0) > 0.0:
            self.loss_TEXT = self.compute_text_similarity_loss(
                self.fake_B, self.text_embeddings
            ) * self.opt.lambda_text
        else:
            self.loss_TEXT = torch.tensor(0.0, requires_grad=True).to(self.device)
            
        # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦æŸå¤±
        if getattr(self.opt, 'lambda_feat', 0.0) > 0.0:
            self.loss_FEAT = self.compute_feature_similarity_loss(
                self.real_B, self.fake_B
            ) * self.opt.lambda_feat
        else:
            self.loss_FEAT = torch.tensor(0.0, requires_grad=True).to(self.device)
            
        # æ€»æŸå¤±
        self.loss_G = loss_G + self.loss_TEXT + self.loss_FEAT
        return self.loss_G

    def get_current_visuals(self):
        """è¿”å›å½“å‰çš„å¯è§†åŒ–ç»“æœ"""
        visual_ret = super().get_current_visuals()
        
        # æ·»åŠ æ–‡æœ¬æè¿°åˆ°å¯è§†åŒ–ä¿¡æ¯ä¸­
        if hasattr(self, 'text_descriptions') and self.text_descriptions:
            visual_ret['text_descriptions'] = self.text_descriptions[:4]  # åªæ˜¾ç¤ºå‰4ä¸ª
            
        return visual_ret
    
    def optimize_memory(self):
        """
        ä¼˜åŒ–GPUå†…å­˜ä½¿ç”¨
        åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸè°ƒç”¨å¯ä»¥é‡Šæ”¾æœªä½¿ç”¨çš„ç¼“å­˜
        """
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            
    def get_model_info(self):
        """
        è¿”å›æ¨¡å‹ä¿¡æ¯ï¼Œç”¨äºè°ƒè¯•å’Œç›‘æ§
        """
        info = {
            'mode': 'train' if self.isTrain else 'test',
            'num_gpus': len(self.opt.gpu_ids),
            'conch_device': str(self.conch_device) if self.conch_device is not None else 'None (test mode)',
            'conch_is_parallel': isinstance(self.conch_model, torch.nn.DataParallel) if self.conch_model is not None else False,
            'text_embeddings_shape': self.text_embeddings.shape if self.text_embeddings is not None else None,
            'lambda_text': getattr(self.opt, 'lambda_text', 0.0),
            'lambda_feat': getattr(self.opt, 'lambda_feat', 0.0),
        }
        return info